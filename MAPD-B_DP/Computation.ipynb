{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Management and Analysis of Physics Dataset - mod.B\n",
    "\n",
    "## Final project: Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a particle physics detector and publish the results in a dashboard for live monitoring.\n",
    "\n",
    "### Students:\n",
    "* Conforto Filippo (2021856)\n",
    "* Domenichetti Lorenzo (2011653)\n",
    "* Faorlin Tommaso (2021857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries cell\n",
    "\n",
    "import json\n",
    "import time\n",
    "import findspark\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "from numpy import linspace \n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, max, min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Spark context and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialisation of spark from the packages folder\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start session - specify port, application name, and configuration settings.\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"MAPD Final Project session\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#default parallelism setting to shuffle different partitions between workers (for join operation).\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism) #15 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MAPD Final Project session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcfe0ea2160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check whether the session is running.\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka streaming setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the kafka server from IP and Port\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"slave04:9092\" \n",
    "\n",
    "#define the input dataframe and its source. Define subscription to topic_stream - one of the two topics in kafka\n",
    "inputDF = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "        .option('subscribe', 'topic_stream')\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the schema of the rows that will be read. double are used to overcome overflow issues\n",
    "schema = StructType(\n",
    "        [StructField(\"HEAD\",        IntegerType()),\n",
    "         StructField(\"FPGA\",         IntegerType()),\n",
    "         StructField(\"TDC_CHANNEL\",  IntegerType()),\n",
    "         StructField(\"ORBIT_CNT\",    DoubleType()),\n",
    "         StructField(\"BX_COUNTER\",   DoubleType()), \n",
    "         StructField(\"TDC_MEAS\",    DoubleType() )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert input_Df to json by casting columns into the predefined schema.\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"), schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattening the dataframe\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First cleanup of streaming dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean dataframe, removing ancillary hits\n",
    "df = flatDF.where(col(\"HEAD\")==2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output streaming service setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scintillator time offset by Chamber\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}\n",
    "\n",
    "#bins definition for histograms - they will be shared among all iterations.\n",
    "binning = list(linspace(0, 4e8, 100))\n",
    "\n",
    "binning_drift = list(linspace(0, 800, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_proc(batch_df, epoch_id):\n",
    "    \n",
    "    #repartition the df DataFrame to 105 parts - and persist in cache to speedup calculations\n",
    "    batch_df.coalesce(15)\n",
    "    batch_df.persist()\n",
    "    \n",
    "    #Dividing the dataframe between chambers\n",
    "    batch_df_ch0 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "    batch_df_ch1 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "    batch_df_ch2 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "    batch_df_ch3 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "    \n",
    "    #coalesce + persist for each filtered batch\n",
    "    batch_df_ch0.coalesce(15)\n",
    "    batch_df_ch1.coalesce(15)\n",
    "    batch_df_ch2.coalesce(15)\n",
    "    batch_df_ch3.coalesce(15)\n",
    "    batch_df_ch0.persist()\n",
    "    batch_df_ch1.persist()\n",
    "    batch_df_ch2.persist()\n",
    "    batch_df_ch3.persist()    \n",
    "       \n",
    "    #list of dfs_ handy for loops\n",
    "    batch_dfs = [batch_df_ch0, batch_df_ch1, batch_df_ch2, batch_df_ch3]\n",
    "    \n",
    "    # Counting hits for each chamber\n",
    "    hits_ch0 = batch_df_ch0.count()\n",
    "    hits_ch1 = batch_df_ch1.count()\n",
    "    hits_ch2 = batch_df_ch2.count()\n",
    "    hits_ch3 = batch_df_ch3.count()\n",
    "    \n",
    "    #total number of hits as the sum of the previous 4 operations\n",
    "    hits = hits_ch0 + hits_ch1 + hits_ch2 + hits_ch3\n",
    "    \n",
    "    if hits!=0: \n",
    "\n",
    "        #Dataframe containing only informations about scintillator events\n",
    "        batch_df_scint = (batch_df.filter('(FPGA==1) AND (TDC_CHANNEL == 128)')\n",
    "                          .select(['ORBIT_CNT', 'BX_COUNTER',\"TDC_MEAS\"])\n",
    "                          .groupBy('ORBIT_CNT')\n",
    "                          .min()\n",
    "                          .withColumnRenamed(\"min(BX_COUNTER)\", 'BX_COUNTER_SCINT')\n",
    "                          .withColumnRenamed(\"min(TDC_MEAS)\", 'TDC_MEAS_SCINT')\n",
    "                          .drop('min(ORBIT_CNT)')\n",
    "                         )\n",
    "        \n",
    "        #optional persist also for the batch_df_scint DataFrame - no need to cache performance-wise\n",
    "        #batch_df_scint.persist()\n",
    "        \n",
    "        #Total active channels histogram\n",
    "        hist1 = {}\n",
    "        \n",
    "        #Channels per orbit histogram \n",
    "        hist2 = {}\n",
    "\n",
    "        #Active channels in orbits in which the scintillator is active histogram\n",
    "        hist3 = {}\n",
    "\n",
    "        #Drifttime histogram\n",
    "        hist4 = {}\n",
    "        \n",
    "        #loop over the four chambers\n",
    "        for chamber in [0,1,2,3]:\n",
    "            #create an empty dictionary for each type of histogram corresponding to the selected chamber\n",
    "            hist1[chamber] = {}\n",
    "            hist2[chamber] = {}\n",
    "            hist3[chamber] = {}\n",
    "            hist4[chamber] = {}\n",
    "\n",
    "            #TDC_channel simple histogram - adaptive binning depending on the corresponding channels\n",
    "            bins, counts = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "            \n",
    "            #Count number of active channels per orbit\n",
    "            bins2, counts2 = (\n",
    "                batch_dfs[chamber].groupBy(\"ORBIT_CNT\",\"TDC_CHANNEL\").count()\n",
    "                .select('ORBIT_CNT')\n",
    "                .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "                .histogram(binning)\n",
    "            )\n",
    "            \n",
    "            #Filtering only useful hits (avoid scintillators) and use inner join to consider coincident events\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].join(batch_df_scint, [\"ORBIT_CNT\"], \"inner\")\n",
    "            \n",
    "            #Creating driftime and select only positive values\n",
    "            #Values smaller than zero should be artifacts - only a few percentage\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].withColumn('DRIFTIME', 25*((col('BX_COUNTER')-col('BX_COUNTER_SCINT'))+(col('TDC_MEAS')-col('TDC_MEAS_SCINT'))/30)+time_offset_by_chamber[chamber])\\\n",
    "                                .where(col('DRIFTIME')>0) \n",
    "            \n",
    "            #optional --- persist is not needed also in this case\n",
    "            #batch_dfs[chamber].persist()\n",
    "            \n",
    "            #TDC_CHANNEL histogram after scintillator selection\n",
    "            bins3, counts3 = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "\n",
    "            #histogram for drifttime - a \"box\" is expected\n",
    "            bins4, counts4 = (\n",
    "                batch_dfs[chamber].select('DRIFTIME')\n",
    "                .rdd.map(lambda x: x.DRIFTIME)\n",
    "                .histogram(binning_drift)\n",
    "            )\n",
    "            \n",
    "            \n",
    "            #convert to python integers both bins and counts \n",
    "            hist1[chamber]['bins'] = list(map(float,bins)) \n",
    "            hist1[chamber]['counts'] = list(map(int,counts))\n",
    "\n",
    "            hist2[chamber]['bins'] = list(map(float,bins2))\n",
    "            hist2[chamber]['counts'] = list(map(int,counts2))\n",
    "\n",
    "            hist3[chamber]['bins'] = list(map(float,bins3))\n",
    "            hist3[chamber]['counts'] = list(map(int,counts3))\n",
    "\n",
    "            hist4[chamber]['bins'] = list(map(float,bins4))\n",
    "            hist4[chamber]['counts'] = list(map(int,counts4))\n",
    "\n",
    "        #producing the results dictionary\n",
    "        result = {\n",
    "            \"hits\" : hits,\n",
    "            \"hits_per_chamber\": [hits_ch0, hits_ch1, hits_ch2, hits_ch3],\n",
    "            \"hist_1\": hist1,\n",
    "            \"hist_2\": hist2,\n",
    "            \"hist_3\": hist3,\n",
    "            \"hist_4\": hist4\n",
    "        }\n",
    "\n",
    "        #sending the json to the producer\n",
    "        producer.send('topic_results', json.dumps(result).encode('utf-8'))\n",
    "\n",
    "        #unpersist DataFrames and free resources\n",
    "        batch_df.unpersist()\n",
    "        batch_df_ch0.unpersist()\n",
    "        batch_df_ch1.unpersist()\n",
    "        batch_df_ch2.unpersist()\n",
    "        batch_df_ch3.unpersist() \n",
    "        \n",
    "    else: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#producer definition from IP address given before\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#process each batch as a WriteStream - 5 seconds batch - rate 1kHz \n",
    "df.writeStream\\\n",
    "    .foreachBatch(batch_proc)\\\n",
    "    .trigger(processingTime='5 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
