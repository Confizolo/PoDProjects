{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603e5f2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MAPDb - Final Project\n",
    "\n",
    "## Streaming processing of cosmic rays using Drift Tubes detectors\n",
    "\n",
    "The goal of this project is to reproduce a real-time processing of real data collected in a particle physics detector and publish the results in a dashboard for live monitoring.\n",
    "\n",
    "### Students:\n",
    "* Conforto Filippo (2021856)\n",
    "* Domenichetti Lorenzo (2011653)\n",
    "* Faorlin Tommaso (2021857)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a0049",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "* Spark cluster architecture\n",
    "* Kafka server setup\n",
    "* Producer\n",
    "* Computation\n",
    "* Dashboard\n",
    "* Benchmarking\n",
    "* Live test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919b72e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736dd411",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"Pictures/cluster.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7116b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why we choose this architecture. After ```cp spark-defaults.conf.template spark-defaults.conf``` we modify this file adding the following lines:\n",
    "\n",
    "```\n",
    "spark.executor.memory          1800m\n",
    "spark.executor.instances          15\n",
    "spark.executor.cores               1\n",
    "```\n",
    "In the end we will instantiate in total **15 executors** (3 for each Worker node) with **one cores each** and **1800 mebibytes** (~1.89 GB) of RAM. We also tried with 10 executors and two cores each, 5 executors and three cores each, and in the end we obtain more or less the same performances in the long run (after the initial 'stabilization phase'). \n",
    "\n",
    "We chose in the end the first configuration to be sure we are leaving one core and $\\approx1$ GB of RAM free for other VMs operations, and to prevent spikes in the processing time due to dead executors. We also notice that leaving one core free allows to have a more stable input rate.\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf92ac3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc6771",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c80f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#initialisation of spark from the packages folder\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "#start session - specify port, application name, and configuration settings\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://master:7077\")\\\n",
    "    .appName(\"MAPD Final Project session\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "#default parallelism setting to shuffle different partitions between workers\n",
    "#after join or groupBy operations\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ff611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kafka server setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de666ed4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Kafka server will be fired up with the script in the next slide.\n",
    "\n",
    "The two Kafka topics *topic_stream* and *topic_results* are created with a **single** partition. \n",
    "\n",
    "We have also tried to increase this number (```num_partitions=2``` and ```num_partitions=4```) without having any improvement in the final batch processing rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62d5aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## One script to rule them all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd5883",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The cluster is created using a single script: ```/home/MAPD_project/Scripts/start-all.sh``` after exchanging ssh keys between master and slaves and setting up spark configuration.\n",
    "\n",
    "\n",
    "```#! /bin/bash```\n",
    "\n",
    "```sh /usr/local/spark/sbin/start-all.sh```\n",
    "\n",
    "```sleep 1```\n",
    "\n",
    "```ssh slave04 sh /home/packages/kafka_2.13-2.7.0/bin/zookeeper-server-start.sh /home/packages/kafka_2.13-2.7.0/config/zookeeper.properties &```\n",
    "\n",
    "```sleep 1```\n",
    "\n",
    "```ssh slave04 sh /home/packages/kafka_2.13-2.7.0/bin/kafka-server-start.sh /home/packages/kafka_2.13-2.7.0/config/server.properties &```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98a2e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d6ca0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#SSL context to dowload without errors data from the given server\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#define the kafka server from IP and Port\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'slave04:9092'\n",
    "\n",
    "#producer definition from IP address given before\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "\n",
    "for i in range(0, 81):\n",
    "    #data download from s3 bucket\n",
    "    i = str(i).zfill(2)\n",
    "    url = f\"https://cloud-areapd.pd.infn.it:5210/swift/v1/\\\n",
    "            AUTH_d2e941ce4b324467b6b3d467a923a9bc/MAPD_miniDT_stream/data_0000{i}.txt\"\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    #data cleaning for possible outliers\n",
    "    df = df[df.ORBIT_CNT < 5e8]\n",
    "    print(f\"Reading file data_0000{i}.txt\")\n",
    "    \n",
    "    #for loop over file size\n",
    "    for j in tqdm(range(0, df.shape[0])):\n",
    "        \n",
    "        #dictionaries creation from dataframe's rows\n",
    "        jj = df.iloc[j].to_dict()\n",
    "        \n",
    "        #unnecessary floats are cast into ints\n",
    "        for key in ['HEAD', 'FPGA', \"TDC_CHANNEL\"]:\n",
    "            jj[key] = int(jj[key])\n",
    "            \n",
    "        #json row is sent to the Kafka topic 'topic_stream'\n",
    "        producer.send('topic_stream',\n",
    "                      json.dumps(jj).encode('utf-8')\n",
    "                     )\n",
    "        time.sleep(0.0003)\n",
    "        \n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13edc4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e5a8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4aa48f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#define the kafka server from IP and Port\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"slave04:9092\" #on the fourth VM \n",
    "\n",
    "#define the input dataframe and its source. Define subscription to 'topic_stream'\n",
    "inputDF = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\\\n",
    "        .option('subscribe', 'topic_stream')\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d5aac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#define the schema of the rows that will be read\n",
    "#double are used to overcome overflow issues\n",
    "schema = StructType(\n",
    "        [StructField(\"HEAD\",        IntegerType()),\n",
    "         StructField(\"FPGA\",         IntegerType()),\n",
    "         StructField(\"TDC_CHANNEL\",  IntegerType()),\n",
    "         StructField(\"ORBIT_CNT\",    DoubleType()),\n",
    "         StructField(\"BX_COUNTER\",   DoubleType()), \n",
    "         StructField(\"TDC_MEAS\",    DoubleType() )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d6769",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#convert input_Df to json by casting columns into the predefined schema.\n",
    "jsonDF = inputDF.select(from_json(col(\"value\").alias('value').cast(\"string\"),\n",
    "                                  schema).alias('value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3600a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#flattening the dataframe\n",
    "flatDF = jsonDF.selectExpr(\"value.HEAD\", \n",
    "                           \"value.FPGA\", \n",
    "                           \"value.TDC_CHANNEL\",\n",
    "                           \"value.ORBIT_CNT\",\n",
    "                           \"value.BX_COUNTER\",\n",
    "                           \"value.TDC_MEAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893c506",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#clean dataframe, removing ancillary hits\n",
    "df = flatDF.where(col(\"HEAD\")==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9aff1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#scintillator time offset by Chamber\n",
    "time_offset_by_chamber = {\n",
    "0: 95.0 - 1.1, # Ch 0\n",
    "1: 95.0 + 6.4, # Ch 1\n",
    "2: 95.0 + 0.5, # Ch 2\n",
    "3: 95.0 - 2.6, # Ch 3\n",
    "}\n",
    "\n",
    "#bins definition for histograms - they will be shared among all iterations.\n",
    "binning = list(linspace(0, 4e8, 100))\n",
    "\n",
    "binning_drift = list(linspace(0, 800, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138d859",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def batch_proc(batch_df, epoch_id):\n",
    "    \n",
    "    #repartition the df DataFrame to 100 parts\n",
    "    #and persist in cache to speedup calculations\n",
    "    batch_df.coalesce(15)\n",
    "    batch_df.persist()\n",
    "    \n",
    "    #Dividing the dataframe between chambers\n",
    "    batch_df_ch0 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "    batch_df_ch1 = batch_df.filter('(FPGA==0) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "    batch_df_ch2 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 0) AND (TDC_CHANNEL < 64)')\n",
    "    batch_df_ch3 = batch_df.filter('(FPGA==1) AND (TDC_CHANNEL >= 64) AND (TDC_CHANNEL < 128)')\n",
    "    \n",
    "    #coalesce + persist for each filtered batch\n",
    "    batch_df_ch0.coalesce(15)\n",
    "    batch_df_ch1.coalesce(15)\n",
    "    batch_df_ch2.coalesce(15)\n",
    "    batch_df_ch3.coalesce(15)\n",
    "    batch_df_ch0.persist()\n",
    "    batch_df_ch1.persist()\n",
    "    batch_df_ch2.persist()\n",
    "    batch_df_ch3.persist()    \n",
    "       \n",
    "    #list of dfs_ handy for loops\n",
    "    batch_dfs = [batch_df_ch0, batch_df_ch1, batch_df_ch2, batch_df_ch3]\n",
    "    \n",
    "    #counting hits for each chamber\n",
    "    hits_ch0 = batch_df_ch0.count()\n",
    "    hits_ch1 = batch_df_ch1.count()\n",
    "    hits_ch2 = batch_df_ch2.count()\n",
    "    hits_ch3 = batch_df_ch3.count()\n",
    "    \n",
    "    #total number of hits as the sum of the previous 4 operations\n",
    "    hits = hits_ch0 + hits_ch1 + hits_ch2 + hits_ch3\n",
    "    \n",
    "    if hits!=0: \n",
    "\n",
    "        #dataframe containing only informations about scintillator events\n",
    "        batch_df_scint = (batch_df.filter('(FPGA==1) AND (TDC_CHANNEL == 128)')\n",
    "                          .select(['ORBIT_CNT', 'BX_COUNTER',\"TDC_MEAS\"])\n",
    "                          .groupBy('ORBIT_CNT')\n",
    "                          .min()\n",
    "                          .withColumnRenamed(\"min(BX_COUNTER)\", 'BX_COUNTER_SCINT')\n",
    "                          .withColumnRenamed(\"min(TDC_MEAS)\", 'TDC_MEAS_SCINT')\n",
    "                          .drop('min(ORBIT_CNT)')\n",
    "                         )\n",
    "\n",
    "        #total active channels histogram\n",
    "        hist1 = {}\n",
    "        \n",
    "        #channels per orbit histogram \n",
    "        hist2 = {}\n",
    "\n",
    "        #active channels in orbits in which the scintillator is active histogram\n",
    "        hist3 = {}\n",
    "\n",
    "        #drifttime histogram\n",
    "        hist4 = {}\n",
    "        \n",
    "        #loop over the four chambers\n",
    "        for chamber in [0,1,2,3]:\n",
    "            #create an empty dictionary for each type of histogram corresponding to the selected chamber\n",
    "            hist1[chamber] = {}\n",
    "            hist2[chamber] = {}\n",
    "            hist3[chamber] = {}\n",
    "            hist4[chamber] = {}\n",
    "\n",
    "            #TDC_channel simple histogram - adaptive binning depending on the corresponding channels\n",
    "            bins, counts = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "            \n",
    "            #Count number of active channels per orbit\n",
    "            bins2, counts2 = (\n",
    "                batch_dfs[chamber].groupBy(\"ORBIT_CNT\",\"TDC_CHANNEL\").count()\n",
    "                .select('ORBIT_CNT')\n",
    "                .rdd.map(lambda x: x.ORBIT_CNT)\n",
    "                .histogram(binning)\n",
    "            )\n",
    "            \n",
    "            #filtering only useful hits (avoid scintillators) and use inner join to consider coincident events\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].join(batch_df_scint, [\"ORBIT_CNT\"], \"inner\")\n",
    "            #creating driftime and select only positive values\n",
    "            #values smaller than zero should be artifacts - only a few percentage\n",
    "            batch_dfs[chamber] = batch_dfs[chamber].withColumn('DRIFTIME', \n",
    "                                                               25*((col('BX_COUNTER')-\\\n",
    "                                                                    col('BX_COUNTER_SCINT'))+\\\n",
    "                                                                   (col('TDC_MEAS')-\\\n",
    "                                                                    col('TDC_MEAS_SCINT'))/30)+\\\n",
    "                                                               time_offset_by_chamber[chamber])\\\n",
    "                                                   .where(col('DRIFTIME')>0) \n",
    "            \n",
    "            #TDC_CHANNEL histogram after scintillator selection\n",
    "            bins3, counts3 = (\n",
    "                batch_dfs[chamber].select('TDC_CHANNEL')\n",
    "                .rdd.map(lambda x: x.TDC_CHANNEL)\n",
    "                .histogram(list(arange((chamber % 2)*64,(chamber % 2 +1)*64,1)))\n",
    "            )\n",
    "\n",
    "            #histogram for drifttime - a \"box\" is expected\n",
    "            bins4, counts4 = (\n",
    "                batch_dfs[chamber].select('DRIFTIME')\n",
    "                .rdd.map(lambda x: x.DRIFTIME)\n",
    "                .histogram(binning_drift)\n",
    "            )\n",
    "            \n",
    "            #convert to python integers both bins and counts\n",
    "            hist1[chamber]['bins'] = list(map(float,bins)) \n",
    "            hist1[chamber]['counts'] = list(map(int,counts))\n",
    "\n",
    "            hist2[chamber]['bins'] = list(map(float,bins2))\n",
    "            hist2[chamber]['counts'] = list(map(int,counts2))\n",
    "\n",
    "            hist3[chamber]['bins'] = list(map(float,bins3))\n",
    "            hist3[chamber]['counts'] = list(map(int,counts3))\n",
    "\n",
    "            hist4[chamber]['bins'] = list(map(float,bins4))\n",
    "            hist4[chamber]['counts'] = list(map(int,counts4))\n",
    "\n",
    "        #producing the results dictionary\n",
    "        result = {\n",
    "            \"hits\" : hits,\n",
    "            \"hits_per_chamber\": [hits_ch0, hits_ch1, hits_ch2, hits_ch3],\n",
    "            \"hist_1\": hist1,\n",
    "            \"hist_2\": hist2,\n",
    "            \"hist_3\": hist3,\n",
    "            \"hist_4\": hist4\n",
    "        }\n",
    "\n",
    "        #sending the json to the producer\n",
    "        producer.send('topic_results', json.dumps(result).encode('utf-8'))\n",
    "\n",
    "        #unpersist DataFrames and free resources\n",
    "        batch_df.unpersist()\n",
    "        batch_df_ch0.unpersist()\n",
    "        batch_df_ch1.unpersist()\n",
    "        batch_df_ch2.unpersist()\n",
    "        batch_df_ch3.unpersist() \n",
    "        \n",
    "    else: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823adbc1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb92dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#producer definition from IP address given before\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "#process each batch as a WriteStream - 5 seconds batch - rate 1kHz \n",
    "df.writeStream\\\n",
    "    .foreachBatch(batch_proc)\\\n",
    "    .trigger(processingTime='5 second')\\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1431f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e40a01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#define the kafka server from IP and Port\n",
    "KAFKA_BOOTSTRAP_SERVERS='slave04:9092'\n",
    "\n",
    "#consumer definition from IP address given before\n",
    "consumer = KafkaConsumer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS, consumer_timeout_ms=100000)\n",
    "\n",
    "#consumer subscription to topic_results\n",
    "consumer.subscribe('topic_results')\n",
    "\n",
    "#Additional configuration options for consumer\n",
    "consumer.poll(timeout_ms=0,         # do not enable dead-times before one poll to the next\n",
    "              max_records=None,     # do not limit the number of records to consume at once \n",
    "              update_offsets=True   # update the reading offsets on this topic\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb1178",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def rt_plot(js,num, fig,axes):\n",
    "    \n",
    "    #to delete the texts written during the previous update\n",
    "    for txt in fig.texts:\n",
    "        txt.set_visible(False)\n",
    "    \n",
    "    #label with informations on the right side of the window\n",
    "    plt.figtext(1.01, 0.5,\n",
    "                f'Batch N° {num}:\\n\\nHits: {js[\"hits\"]} \\n\\n' +\\\n",
    "                f'Channel 0: {js[\"hits_per_chamber\"][0]} hits\\n\\n' +\\\n",
    "                f'Channel 1: {js[\"hits_per_chamber\"][1]} hits\\n\\n' +\\\n",
    "                f'Channel 2: {js[\"hits_per_chamber\"][2]} hits\\n\\n' +\\\n",
    "                f'Channel 3: {js[\"hits_per_chamber\"][3]} hits' \n",
    "                , ha='left', va='center', fontsize = 20, \n",
    "                bbox=dict(facecolor='snow',\n",
    "                          edgecolor='black',\n",
    "                          boxstyle='round'))\n",
    "    \n",
    "    #main title of the upper panel\n",
    "    plt.figtext(0.5, 1.07,\n",
    "                f'Muon-Hits Monitors',\n",
    "                ha='center', va='center', fontsize = 30)\n",
    "    plt.figtext(0.5, 1.025,\n",
    "                f'Total active channels',\n",
    "                ha='center', va='center', fontsize = 22)\n",
    "\n",
    "    #main title of the second panel\n",
    "    plt.figtext(0.5, 0.75,\n",
    "                'Total number of active channels per orbit',\n",
    "                ha='center', va='center', fontsize = 22)\n",
    "\n",
    "    #main title of the third panel\n",
    "    plt.figtext(0.5, 0.5,\n",
    "                f'Total active channels for orbits with active scintillator',\n",
    "                ha='center', va='center', fontsize = 22)\n",
    "                \n",
    "    #main title of the lower panel\n",
    "    plt.figtext(0.5, 0.24,\n",
    "                'Drift time',\n",
    "                ha='center', va='center', fontsize = 22)\n",
    "    \n",
    "    for j,xlab,histo in zip([0,1,2,3],\n",
    "                            [\"Channel N°\",\"Orbit N°\",\"Channel N°\",\"Time[ns]\"],\n",
    "                            [\"hist_1\", \"hist_2\", \"hist_3\",\"hist_4\"]):\n",
    "\n",
    "        for i, ax in enumerate(axes[j]):\n",
    "            #clean previous plots and update titles and labels\n",
    "            ax.clear()\n",
    "            ax.set_title(f'Chamber {i}', fontsize=15)\n",
    "            ax.set_xlabel(xlab,  fontsize=14)\n",
    "            if i==0:\n",
    "                ax.set_ylabel(\"Counts\",  fontsize=14)\n",
    "            #bins centers computation\n",
    "            bin_centers = js[histo][str(i)][\"bins\"][:-1] + np.diff(js[histo][str(i)][\"bins\"])/2\n",
    "            ax.hist(bin_centers, weights=js[histo][str(i)][\"counts\"]\n",
    "                    , bins=js[histo][str(i)][\"bins\"], alpha=0.6)\n",
    "            ax.tick_params(labelsize=13)\n",
    "\n",
    "    #to separate better the plots\n",
    "    plt.tight_layout(h_pad = 7, w_pad = 1)\n",
    "    \n",
    "    #clean the whole screen and update with new incoming data\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "\n",
    "def bin_sum(old_j,new_j):\n",
    "    #updating old values \n",
    "    for i,name in enumerate([\"hits\",\n",
    "                             \"hits_per_chamber\",\n",
    "                             \"hist_1\",\n",
    "                             \"hist_2\",\n",
    "                             \"hist_3\",\n",
    "                             \"hist_4\"]):\n",
    "            if i==0:\n",
    "                #to add new hits\n",
    "                old_j[name]+=new_j[name]\n",
    "            elif i==1:\n",
    "                #to add new hits per chamber\n",
    "                old_j[name]=np.asarray(old_j[name]) +\\\n",
    "                np.asarray(new_j[name])\n",
    "            else:\n",
    "                for j in range(4):\n",
    "                    #to update counts with the incoming data array\n",
    "                    old_j[name][str(j)][\"counts\"]=np.asarray(old_j[name][str(j)][\"counts\"]) +\\\n",
    "                                                  np.asarray(new_j[name][str(j)][\"counts\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107ef28",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e00dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cumulative dashboard service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba483c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Figure creation\n",
    "fig, axes = plt.subplots(4,4, figsize = (15,20), sharey = 'row')\n",
    "for num,message in enumerate(consumer):\n",
    "    if num==0:\n",
    "        #The first incoming message is simply stored\n",
    "        msg = json.loads(message.value)\n",
    "    else: \n",
    "        #The messages following are obtained as an update of the first one\n",
    "        bin_sum(msg,json.loads(message.value))\n",
    "    #plot production\n",
    "    rt_plot(msg,num,fig,axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9bbf63",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"Pictures/graphs.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac661426",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c14b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Single batch analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae339a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Figure creation\n",
    "fig, axes = plt.subplots(4,4, figsize = (15,20), sharey = 'row')\n",
    "for num,message in enumerate(consumer):\n",
    "    #Creating and plotting values for the received message\n",
    "    msg = json.loads(message.value)\n",
    "    rt_plot(msg,num,fig,axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2313a39",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"Pictures/graph2.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e98f2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efe80c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdbab64",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Performance analysis on the WebUI (localhost:4040)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77d0f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"Pictures/webui_perfromances.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "**Batch duration**\n",
    "\n",
    "After an initial transient the processing time stabilizes below 5s. This behaviour is probably due to a progressive elaboration of collected rows after a first phase of works deployment.\n",
    "\n",
    "**Input and process rate**\n",
    "\n",
    "The input rate seems to be stabilized around the expected rate (~1000 rows/s), as also the input rows. The high process rate also allow to compute the incoming batch in the required time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1f77c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397d7f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Horizontal scaling\n",
    "\n",
    "The following times are taken when the batch duration time stabilizes (approximately, at sight) for a $1$ kHz input rate:\n",
    "\n",
    "* 5 workers - 15 executors: Batch duration ~3.3 s.\n",
    "* 4 workers - 12 executors: Batch duration ~3.6 s.\n",
    "* 3 workers - 9 executors: Batch duration  ~3.8 s. (we note an instability)\n",
    "* 2 workers - 6 executors: Batch duration  ~4.0 s. \n",
    "\n",
    "By scaling horizontally the cluster, is not possible to appreciate consistent differencies in the processing time. This is due to the fact that for a small number of data we do not have an advantage in using distributed calculus. With larger input rates we could have a better understanding of horizontal scaling and this phenomenon.\n",
    "\n",
    "### Vertical scaling\n",
    "\n",
    "* 5 workers - 5 executors (6000 mb of RAM and 3 cores each) ~3.0s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27099f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ae146",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coalesce (Repatition)\n",
    "\n",
    "```batch_df.coalesce(15)```\n",
    "\n",
    "We use the function ```coalesce()``` rather than ```repartition()``` because this command avoids full shuffles of a given dataset. We changed the number of repartitions and looked at the final performance. In detail, setting the coalesce parameter at $15$, $30$, $60$, $105$ does not affect much the final processing rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066d69b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shuffle partition\n",
    "\n",
    "When changing the ```spark.sql.shuffle.partitions``` we observe the following. The default parameter is $15$ and if we increase in generale this value to multiples of the latter we get worse results. We suppose that this is related to the fact that with more shuffling, the cluster has to exchange more data among different Workers. In numbers:\n",
    "\n",
    "* 15 shuffle partitions ~3.3 s\n",
    "* 30 shuffle partitions ~7 s\n",
    "* 60 shuffle partitions ~12 s\n",
    "\n",
    "If we instead use a lower number than the default one, the computations are carried out with a higher processing time ~4 s. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150527e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scaling with the input rate (kHz)\n",
    "\n",
    "* 1 kHz - with 5 workers (15 executors): ~3.3 s\n",
    "* 3 kHz - with 5 workers (15 executors): ~3.8 s\n",
    "* 10 kHz - with 4 workers (12 executors): ~5.6 s \n",
    "\n",
    "In the last case, in order to maintain a stable data rate, we needed to free slave04 and dedicate all the resources to the Kafka Broker and the Producer (like it was a real Data Acquisition System). \n",
    "\n",
    "The scaling is not linear with the input rate. This is positive, because we can work with a lot of data and obtain results in a reasonable amount of time. Processing more data all together allows to exploit all the power of distributed computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e8c4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbae70f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the end, we have been able to develop a working Structured Streaming application where a DAQ is sending nearly 1000 samples per second to the Spark Master. This project allowed us to work with Big Data processing architectures for the first times, solve the problems related to Apache Kafka and Spark usage and spend quite some time in the tuning of the architecture."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
