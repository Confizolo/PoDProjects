{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc414240",
   "metadata": {
    "id": "-RW-LWJ2gbrh"
   },
   "source": [
    "#NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n",
    "\n",
    "\n",
    "# Homework 3 - Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912943c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment modules\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch import nn\n",
    "from tqdm import tqdm \n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# self-made modules\n",
    "from models import DQN, ReplayMemory, train_loop_pole, train_loop_lander, test_loop\n",
    "from utils import wrap_env, show_videos, exp_prof, exp_prof_egr\n",
    "from policies import choose_action_softmax, choose_action_epsilon_greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7db4fa",
   "metadata": {
    "id": "zjKmdFhowRbj"
   },
   "source": [
    "# Gym Environment (CartPole-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('CartPole-v1') \n",
    "env.seed(0)\n",
    "\n",
    "# get state and action space dimension\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fad734",
   "metadata": {
    "id": "2k3ZQuh8xHo7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize replay memory\n",
    "replay_mem = ReplayMemory(10000)    \n",
    "\n",
    "# initialize the networks\n",
    "policy_net = DQN(state_space_dim, action_space_dim)\n",
    "\n",
    "target_net = DQN(state_space_dim, action_space_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a44b24",
   "metadata": {},
   "source": [
    "# Hyperparameters definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    " \n",
    "# exploration profiles for softmax and egreedy\n",
    "exploration_profile_egr = exp_prof_egr(0.5,500)\n",
    "exploration_profile = exp_prof(6,400)\n",
    "\n",
    "# hyperparameters definition for training\n",
    "hyper = { \"bad_state_penalty\": -0.7,\n",
    "    \"min_samples_for_training\" : 500,\n",
    "    \"gamma\" : 0.97,\n",
    "    \"optimizer\" : torch.optim.Adam,\n",
    "    \"lr\" : np.linspace(1e-3,1e-4,500),\n",
    "    \"loss_fn\" : nn.SmoothL1Loss(),\n",
    "    \"batch_size\" : 64,\n",
    "    \"target_net_update_steps\" : 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e29fa",
   "metadata": {
    "id": "WF6Zf53FoRDZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training Loop start\n",
    "policy_net, target_net, rewards = train_loop(env,policy_net,target_net,exploration_profile_egr,\n",
    " choose_action_epsilon_greedy, replay_mem,\n",
    "  hyper, pen = lambda x,y : -np.abs(x[0]),\n",
    "  early_stopping_pars= [10,490]\n",
    "  )\n",
    "\n",
    "# policy_net.load_state_dict(torch.load(\"params/gym_policy.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"params/gym_cart_egreedy.pth\")\n",
    "#torch.save(policy_net.state_dict(), \"params/gym_cart_softmax.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"params/rewards_softmax\",rewards)\n",
    "# np.save(\"params/rewards_egreedy\",rewards)\n",
    "scores_softmax = np.load(\"params/rewards_softmax.npy\")\n",
    "scores_egreedy = np.load(\"params/rewards_egreedy.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c24dd",
   "metadata": {},
   "source": [
    "# Training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the scores for the softmax exploration\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "\n",
    "plt.title(\"Softmax policy training\")\n",
    "plt.xlabel('Episode')\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "sns.lineplot(x=np.arange(len(scores_softmax)), y = scores_softmax/500, label=\"Score/MAX\", markers= True,ax=ax2, color=\"orange\", legend=False)\n",
    "sns.lineplot(x=np.arange(len(scores_softmax)), y = exploration_profile, label=\"Temperature\", markers= True, ax=ax, legend=False)\n",
    "\n",
    "ax.figure.legend()\n",
    "plt.savefig(\"images/pole_softmax_rewards.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the scores for the egreedy exploration\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "\n",
    "plt.title(\"Egreedy policy training\")\n",
    "plt.xlabel('Episode')\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "sns.lineplot(x=np.arange(len(scores_egreedy)), y = scores_egreedy/500, label=\"Score/MAX\", markers= True, ax=ax2, color=\"orange\", legend=False)\n",
    "sns.lineplot(x=np.arange(len(scores_egreedy)), y = exploration_profile_egr, label=\"Epsilon\", markers= True, ax= ax, legend=False)\n",
    "\n",
    "ax.figure.legend()\n",
    "\n",
    "plt.savefig(\"images/pole_egreedy_rewards.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a082dc",
   "metadata": {
    "id": "JkG9iDZTIhzc"
   },
   "source": [
    "## Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a2720",
   "metadata": {
    "id": "mxdtGJutLqlw"
   },
   "outputs": [],
   "source": [
    "test_loop(env, choose_action_softmax, policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda6702",
   "metadata": {
    "id": "zjKmdFhowRbj"
   },
   "source": [
    "# LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45850bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "env = gym.make('LunarLander-v2') \n",
    "env.seed(0)\n",
    "\n",
    "# get state and action space dimension\n",
    "state_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ddb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay memory\n",
    "replay_mem = ReplayMemory(30000)    \n",
    "\n",
    "# initialize the networks\n",
    "policy_net = DQN(state_space_dim, action_space_dim)\n",
    "\n",
    "target_net = DQN(state_space_dim, action_space_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd818da",
   "metadata": {},
   "source": [
    "## Hyperparameters and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc30600",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# creating exploration profiles\n",
    "exploration_profile_egr = exp_prof_egr(0.5,800)\n",
    "exploration_profile = exp_prof(1,800)\n",
    "\n",
    "# hyperparameters definition\n",
    "hyper = { \"min_samples_for_training\" : 500,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"optimizer\" : torch.optim.Adam,\n",
    "    \"lr\" : np.linspace(2e-4,1e-4,1200),\n",
    "    \"loss_fn\" : nn.SmoothL1Loss(),\n",
    "    \"batch_size\" : 8,\n",
    "    \"target_net_update_steps\" : 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118d31b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start training process\n",
    "policy_net, target_net, scores = train_loop_lander(env,policy_net,target_net,exploration_profile_egr, \n",
    "        choose_action_epsilon_greedy, replay_mem, hyper,\n",
    "        early_stopping_pars= [10,240], pen = lambda x,y : 0, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(policy_net.state_dict(), \"params/lander_egreedy2.pth\")\n",
    "# torch.save(policy_net.state_dict(), \"params/lander_softmax.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982cb560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"params/lander_rewards_softmax\",scores)\n",
    "# np.save(\"params/lander_rewards_egreedy2\",scores)\n",
    "scores_softmax = np.load(\"params/lander_rewards_softmax.npy\")\n",
    "scores_egreedy = np.load(\"params/lander_rewards_egreedy.npy\")\n",
    "scores_egreedy2 = np.load(\"params/lander_rewards_egreedy2.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372f37f",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39447446",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "sns.lineplot(x=np.arange(len(scores_softmax)), y = scores_softmax, label=\"Score\", markers= True,ax=ax2, color=\"orange\", legend=False)\n",
    "sns.lineplot(x=np.arange(len(scores_softmax)), y = exploration_profile, label=\"Temperature\", markers= True, ax=ax, legend=False)\n",
    "\n",
    "ax.figure.legend()\n",
    "plt.title(\"Softmax policy training\")\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008b630",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "sns.lineplot(x=np.arange(len(scores_egreedy2)), y = scores_egreedy2, label=\"Score\", markers= True, ax=ax2, color=\"orange\", legend=False)\n",
    "sns.lineplot(x=np.arange(len(scores_egreedy2)), y = exploration_profile_egr, label=\"Epsilon\", markers= True, ax= ax, legend=False)\n",
    "\n",
    "ax.figure.legend()\n",
    "plt.title(\"Egreedy policy training\")\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecb88d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(10,7))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "sns.lineplot(x=np.arange(len(scores_egreedy)), y = scores_egreedy, label=\"Score\", markers= True, ax=ax2, color=\"orange\", legend=False)\n",
    "sns.lineplot(x=np.arange(len(scores_egreedy)), y = exploration_profile_egr, label=\"Epsilon\", markers= True, ax= ax, legend=False)\n",
    "\n",
    "ax.figure.legend()\n",
    "plt.title(\"Egreedy policy training with early stopping\")\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb4ae8",
   "metadata": {},
   "source": [
    "## Test loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbac289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = wrap_env(env, video_callable=lambda episode_id: True) # save a video every episode\n",
    "\n",
    "test_loop(env, choose_action_softmax, policy_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741cb1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_videos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm video/*"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31210159afb42987aedac289936b2076a3267908284e408b6a3fe9928568de67"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
