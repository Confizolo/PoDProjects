{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9699ee11",
   "metadata": {},
   "source": [
    "# NEURAL NETWORKS AND DEEP LEARNING\n",
    "\n",
    "---\n",
    "A.A. 2021/22 (6 CFU) - Dr. Alberto Testolin, Dr. Umberto Michieli\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336b6e9",
   "metadata": {},
   "source": [
    "# Homework 2 - Autoencoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d218ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as toptim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# sklearn and skorch modules\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skorch import NeuralNetRegressor,NeuralNetClassifier,callbacks\n",
    "\n",
    "# various modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# optuna import\n",
    "import optuna\n",
    "\n",
    "# self-made libraries import\n",
    "from encoders import Encoder\n",
    "from decoders import Decoder\n",
    "from training_func import training_loop, training_loop_sup, training_loop_autoenc, training_loop_den\n",
    "from optimization import objective\n",
    "from utilities import plot_result, add_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b61d307e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utilities' from 'c:\\\\Users\\\\Filippo\\\\Documents\\\\POD\\\\Projects\\\\NNDL\\\\HW2\\\\utilities.py'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, importlib\n",
    "importlib.reload(sys.modules['encoders'])\n",
    "importlib.reload(sys.modules[\"training_func\"])\n",
    "importlib.reload(sys.modules[\"optimization\"])\n",
    "importlib.reload(sys.modules[\"utilities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512dcc6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d13b4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the transformations list\n",
    "transformations = torchvision.transforms.Compose([torchvision.transforms.RandomAffine(degrees=15, translate=(.05, .05), scale=(0.95,1.05)) , transforms.ToTensor() ])\n",
    "\n",
    "# loading train and test dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST('classifier_data', train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.FashionMNIST('classifier_data', train=False, download=True)\n",
    "\n",
    "train_dataset.transform = transformations\n",
    "test_dataset.transform = transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b242a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define train dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "### Define test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabae1a",
   "metadata": {},
   "source": [
    "## Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c6355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "device = \"cpu\"\n",
    "### Initialize the two networks\n",
    "encoded_space_dim = 2\n",
    "encoder = Encoder(encoded_space_dim=encoded_space_dim,device=device)\n",
    "decoder = Decoder(encoded_space_dim=encoded_space_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f040d7c",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5e89d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr = 5e-4 # Learning rate\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "511da1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 1/10 - loss: 0.065332\n",
      "\n",
      "\n",
      "EPOCH 2/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 2/10 - loss: 0.053759\n",
      "\n",
      "\n",
      "EPOCH 3/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 3/10 - loss: 0.048455\n",
      "\n",
      "\n",
      "EPOCH 4/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 4/10 - loss: 0.046704\n",
      "\n",
      "\n",
      "EPOCH 5/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 5/10 - loss: 0.045746\n",
      "\n",
      "\n",
      "EPOCH 6/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 6/10 - loss: 0.045304\n",
      "\n",
      "\n",
      "EPOCH 7/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 7/10 - loss: 0.044695\n",
      "\n",
      "\n",
      "EPOCH 8/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 8/10 - loss: 0.044370\n",
      "\n",
      "\n",
      "EPOCH 9/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 9/10 - loss: 0.044150\n",
      "\n",
      "\n",
      "EPOCH 10/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 10/10 - loss: 0.043643\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder,decoder,train_loss,test_loss = training_loop(10,device,encoder,decoder,train_dataloader,test_dataloader,loss_fn,optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df3e63aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1QAAAG2CAYAAAB4csf9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5B0lEQVR4nO3de3xU9Z3/8c8kM7knhEAuICEIgqhc1xsowkNdKJXE27oq8IC11Op2laq17Spry1YfKktpray6u32sWx9b2XoruoKA8MCWitFaVFCqIIiJBDCEe0KSmcnM+f2xv6ZqzpnJvElmAnk9/4LzmU8+3zlz5nzPJ5P5Hp/jOI4BAAAAABKWluoBAAAAAMDJioYKAAAAAEQ0VAAAAAAgoqECAAAAABENFQAAAACIaKgAAAAAQERDhV7l17/+tV155ZV2xRVX2IwZM+z73/++7d271/Px3/rWt2znzp0xf+ajjz5qL730kjymOXPm2Jo1azps/6d/+ierrq6Wfy4AAAC6n4/7UKG3+Jd/+Rfbtm2bPfTQQzZgwACLRqP28ssv209/+lN7/vnnraysLCXjmjNnjs2ePdumT5+ekvoAAADQ+VM9ACAZPv/8c3vmmWfsd7/7nfXp08fMzNLS0uzqq6+2rVu32n/8x3/YwoUL7bLLLrMxY8bY9u3b7bvf/a49/PDD9uijj9ro0aPtF7/4hb3wwguWm5tr5513nq1fv95ee+01u+eee2z48OH2zW9+00aPHm233HKLvfHGG7Z//367+eabbdasWdbc3Gz//M//bLW1tXbkyBHLzc21JUuW2NChQz3H/OdGa9SoUfZ3f/d3dvHFF9vWrVstEonYd77zHXv22Wdt165dNmrUKPvZz35maWlp9u///u+2fv16a21ttZaWFvvHf/xHmzp1qrW0tNjChQtty5Ytlp+fb2eccYaZmS1atMjq6+vt/vvvt3379lk4HLYZM2bY3//93yfldQEAADjZ8Sd/6BW2bNliQ4cObW+mvuiiiy6yd955p/3/w4cPt9WrV9vUqVPbt73++uu2fPlye+GFF2z58uV2/Phx1zqhUMj69u1rzzzzjC1dutQefvhhCwaD9vvf/94KCgrs2WeftVdffdVGjRply5Yt6/T46+rqbMqUKbZ8+XIbN26cPfjgg/azn/3MXnnlFdu0aZNt3rzZ9uzZY9XV1farX/3KVqxYYXfddZctXbrUzMyeeOIJi0Qitnr1anvqqafsww8/bP/Z3//+9+1v/uZv2p9fdXW1rVq1qtNjAwAA6M34hAq9Rltbm+v2UChkPp+v/f/nnXdeh8ds2LDBpk+fbgUFBWZmNnv2bHvrrbdcf97ll19uZmbnnHOOhUIha25utunTp1t5ebn96le/straWnv77bdt/PjxnR57IBCwyy67zMzMBg8ebOPHj7e8vDwzMyspKbGjR4/aX/3VX9nixYttxYoVVltba1u2bGlv/DZs2GD33nuvpaWlWV5enl1zzTW2fft2a25utj/+8Y929OhRe/TRR83MrLm52bZt22ZXXHFFp8cHAADQW9FQoVcYN26c1dbWWkNDgxUXF38p9oc//OFLzU1OTk6HfL/fb1/8umF6erpnrczMTDOz9ibNcRz7n//5H3vuueds9uzZVlVVZYWFhVZXV9fp8QcCgS81fYFAoMNj/vSnP9k//MM/2E033WQXX3yxnX/++fbjH//Ydfxpaf/34XQ0GjXHceyZZ56x7OxsMzM7dOhQ+3MAAABAbPzJH3qF0tJSmzNnjn33u9+1+vr69u2/+c1vbO3atfatb30rZv6UKVNs7dq11tjYaGZmL7zwQkL1N27caNdcc4397d/+rZ1++un22muvWSQSSfyJxPDHP/7RRo0aZd/4xjfsggsusPXr17fXmDJliv3mN7+xaDRqLS0ttnLlSvP5fJaXl2fjxo2zX/7yl2ZmduzYMZs5c6atX7++S8cGAABwquITKvQad999tz3//PP27W9/20KhkIVCIRs9erQ988wzdtppp8XMnThxol1//fV2ww03WFZWlg0fPrz9E53OmDdvnv3oRz9qb8TGjRtnH3/88Qk9n6+qrKy0tWvX2te//nWLRqN26aWX2tGjR62pqcluvfVWu//++62qqsry8/OtX79+lpWVZWZmS5YssQceeMCqqqosFApZZWWlXXnllV06NgAAgFMVy6YDnfDBBx/Ye++9Z3PnzjUzs1/+8pe2ZcsW+/nPf57agXXSK6+8Ynl5eTZlyhSLRqM2f/58u/jii23WrFmpHhoAAMBJjYYK6ISmpiZbsGCB7dq1y3w+nw0YMMAeeOABKy0tTfXQOuXjjz+2H/3oR9bS0mLhcNguvPBCW7Bgget3sQAAANB5NFQAAAAAIGJRCgAAAAAQ0VABAAAAgIiGCgAAAABEKVs2fdKkSa43Nq2pqbEhQ4Ykf0A9HPvFHfulI/aJO/aLu56yXwYNGmQbN25M9TDaMUclhv3iLtZ++eLN2hOhLiYUjUYTzol1E/tY4n09/+OPP7YRI0Z02K7uE/W+jn++yX2iuuu127p1q40aNarD9nA4LNVTx6kur6DWi2Xbtm02cuTILq2XmZmZcM7AgQNt1apVnvGULUoxZMgQq62t7bDdcZxueUFOduwXd+yXjtgn7tgv7nrKfqmoqLCamppUD6Mdc1Ri2C/uYu2X3txQBYNB14vaU72hysjIiBk/duyYFRQUdNgeCoWkeurz60kNVXNzs+Xk5HRpvT/fhzMR5eXltnnzZs84f/IHAAAAAKITaqhWrFhhV1xxhU2bNs2WLVvWVWMCAOCEMUcBAJJB/g5VfX29PfLII7Z8+XLLyMiwG2+80S688EI744wzunJ8AAAkjDkKAJAs8idU1dXVNmHCBCssLLScnBz72te+ZmvWrOnKsQEAIGGOAgAki/wJ1f79+624uLj9/yUlJfb+++93Oj/Wl49TtE5Gj8d+ccd+6Yh94o794u5U3C/MUcnHfnHHfnEXDAZTPYQe6dixY6keQo/T3Nyc6iHEJTdU0Wj0S6trJLrCDysoJYb94o790hH7xB37xV1P2S9dvcofc1RysV/cscqfO1b5c8cqfx2d8qv8lZWVWUNDQ/v/GxoarKSkRP1xAAB0GeYoAECyyA3VRRddZG+++aYdOnTIWlpabO3atTZ58uSuHBsAABLmKABAssh/8ldaWmp33XWXzZ0718LhsF133XU2ZsyYrhwbAAAS5igAQLLIDZWZWVVVlVVVVXXVWAAA6DLMUQCAZDihG/sCAAAAQG92Qp9QAQAA9GbxVhrr6pXP1JXbFOoqf53h93e8BO1pq+51Nbfn/FVuqziqq+6pK0Kqqyaq44xXz+v1VY+Xtra2hHPijZFPqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAET+VA8A6M18Pp+U5zhOF48EAHAySEvTfhfu9yd+yafkdFZGRkaHbTk5OdLPikQiUl4gEJDyotGolJeenh73Mbm5uZ3a1hnqsRIMBpNar62tLWY8Ly/PdXsoFJLqKdde8Z4bn1ABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAEQ0VAAAAAAgoqECAAAAABENFQAAAACI/KkeANCbpaVpv9OIRCJdPJLY8vPzpbxJkyZJeatXr5byVD6fT8pLT0+X8tra2qQ8APGp7+fuqucVV8//2dnZUp7fn/gln1orLy8v7mMGDRok5blxHEfKU8/h6mvXmXP/aaed1mFbNBrttnpu1PeQem3S3NwcM15YWOi6vampSaqnjDMjIyNmnE+oAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAEQ0VAAAAAAgoqECAAAAABENFQAAAACIaKgAAAAAQERDBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARP5UDwDozdLT06W8SCQi5Z1xxhlS3s033yzltbS0SHnHjx+X8lpbW2PGL7jgAtftb7/9tlSvra1NyjvV+Xy+pOR0p8zMTMvKynKNeW1Xjwf1uUejUSlPFa9eV7+G6s9Tz6uqePUyMjJct3sdR/H0799fysvLy0s4p1+/flKt4uLiuI8ZO3Zsh22DBg2S6jmOI+Wpx0pzc7OUl5YW/3OMiRMndtjW1NQk1VM1NjZKeep7Nl69ESNGuG4/fPiwVE+5xoh3TPMJFQAAAACIaKgAAAAAQERDBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgMif6gEAvVkoFEpqvcsuu0zK++u//mspr66uTsrLzMyU8nJycmLGZ82a5bp96tSpUr3//M//lPLq6+ulPJXfH/tU7xVva2uT6jmOk5Sc7hSNRi0SibjGvLYn+zlEo9Gk1ov3/Lr6+as/Ly1N+12xz+eT8uKdr7ziGRkZUr1472cvWVlZCeeUlpZKtU4//fS4jxk2bFiHbcXFxVK9vLw8KU89xrzOAfG0tLTEfYzbvlPf6+o1RjAYlPKampqkvEOHDsWMDxgwwHV7d74OXxXvvMInVAAAAAAgoqECAAAAABENFQAAAACITug7VHPmzLFDhw61/z3v/fffb2PHju2SgQEAcCKYowAAySA3VI7jWE1Njf32t7+VvyAJAEB3YI4CACSL/Cd/u3btMjOzefPm2ZVXXmlPP/10lw0KAIATwRwFAEgWnyOuGfnee+/Zr3/9a/vhD39o4XDY5s6da/fee69dfPHFXT1GAAASwhwFAEgWuaH6qqeeesr27t1rCxYs6NTjhwwZYrW1tR22O44j3wviVMZ+ccd+6SjWPrnlllukn6nmqfeh+vDDD6W8WPehuuOOO+zRRx91jTU0NEj1ToX7UIXDYQsEAq4x9T5UioqKCqupqem2n5/oHDV8+HDXOSoUCnneP0i9V4x6DlPvwaKKdbnQk87F6p94quOPdd45cuSIFRYWusbU++2p92ryGkcsQ4cOlWrFuw/Vj3/8Y1u4cGGH7b39PlTz58+3f/3Xf+2wvTffh+oXv/iF5zWIeo2hzMEDBgywlStXesblP/nbtGmTvfnmm+3/dxyHv1MHAPQIzFEAgGSRG6rGxkZbvHixBYNBa2pqshdffNGmTp3alWMDAEDCHAUASBb513WXXnqpbdmyxa6++mqLRqM2a9YsGz9+fFeODQAACXMUACBZTujvH+6880678847u2goAAB0HeYoAEAyyH/yBwAAAAC9Hd/QBbqIskKUusJQvO+CeMW//e1vS/XKy8ulvPT0dCkvLU37Xc+rr74aM75jxw7X7RdccIFU7/rrr5fyPvjgAynvo48+kvLirWjktZqfunKWsiJVdna2VKu7RCIRz5W8unp1PXUFry5apPeUo+5PddESr1Uy48W9Vos80XpeSkpKEs4ZPny4VCveKn9eP7usrEyqp+5LNS8cDkt5nVk976yzzuqwTV2tT12p9fjx40nNi7fCq9ex29zcLNVrbGxMOCfeHMUnVAAAAAAgoqECAAAAABENFQAAAACIaKgAAAAAQERDBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACJ/qgcAdBefz5fqIXSbBx54QIoPGDBAqtfY2Cjl5ebmSnkjRoyQ8o4ePRozPmHCBNftO3fulOrl5eVJeRMnTpTyrr32Wilv3bp1MeNVVVWu288//3ypXnV1dcI5JSUlUq3u4jiOOY7jGUtkO5JLfR3a2tqkvGAwKMUzMjKkellZWVJefn5+0moVFhZKj+lMnht1Xyb7OiEcDsd9TFFRUYdtkUik2+q5UV+HI0eOSHmhUChmfODAga7bDx482C313MTbl3xCBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABENFQAAAAAIPKnegBAd3EcJ9VD6DaHDx+W4oWFhVK9zMxMKS83N1fKC4VCUt6kSZOk+HnnnSfVi0ajUt67774r5e3cuVPKGzNmjBT/8MMPpXo333xzwjk5OTlSre7iOI7nOeRUPrecCpL9+gSDQSmekZHRHcPxpJyPBw0aJNUqKCiQHtOZPDd+v3Y5m56eLuWpx1gkEon7mD59+nTYlpamff6hjrO1tVXKU68V4tXr37+/6/b8/HypXlFRUcI5bq/LF/EJFQAAAACIaKgAAAAAQERDBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgMif6gEAvdmGDRukvKFDh8aMjxo1ynV7S0uLVK+5uVnK8/u1U0xeXp6U19raGjPuNZ7s7GypXjQalfIuueQSKe+iiy6S8tLSYv/u7Oabb3bdXldXJ9WLd3y6SU9Pl2oBqebz+aS4en7s06ePlFdWVpZwTkZGhlQrPz9feoxaLzMzU8pTzzuO40h54XA47mPcnksgEJDqqeOMN2d4iUQiUl5OTo4UV1+/eNcKboLBYMw4n1ABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAEQ0VAAAAAAgoqECAAAAABENFQAAAACI/KkeABCPz+eT4mlp2u8LIpGIlJeXl5dwzsCBA6VawWBQisfL85KZmSnl7d69W8o7ePCglDdkyBDP2KBBg+zIkSOuMcdxpHrxjk0v6rGZk5Mj5cU7pr3i48aNk+qtXbs24ZycnBybPn26VA/oCur72e+PfSnlFc/NzZXq9e/fX8rLz89POKewsFCqlZ2dLT0mIyNDqhcIBKS89PR0KU89h3dmnG7neXWc6tym1lPFO8684n369JHq9evXL+Gcvn37xozzCRUAAAAAiGioAAAAAEBEQwUAAAAAok41VE1NTVZZWWl1dXVmZlZdXW1VVVU2bdo0e+SRR7p1gAAAxMM8BQBIlbgN1ZYtW2zmzJlWU1NjZmatra22YMECe+KJJ2zVqlW2detW27BhQ3ePEwAAV8xTAIBUittQPffcc7Zw4UIrKSkxM7P333/fKioqrLy83Px+v1VVVdmaNWu6faAAALhhngIApFLcZdMffPDBL/1///79Vlxc3P7/kpISq6+vT7jwn3+T6EZd5vFUx35xF41GUz2EHmfYsGGpHoKZmQ0fPjzVQ/iSUaNGpXoIPVJXHy/XXnttl/68eLpjnmKOShz7xV1TU1Oqh9AjjRw5MtVD6JGKiopSPQTZF8+7iYh1yxMzs0mTJiW0PRUSvg9VNBr90r0aHMeR7t0wZMgQq62t7bBd/Xmnut68X2I972g06nk/iJPhPlTvvfeeVCvWPhk2bJh98sknrrGWlhapnnofqgMHDkh53XEfqlGjRtnWrVtdY735PlSxjpfS0lKpXqrvQ9UV8xRzVGJOhf2ivi9j3XOpqanJc25Q318TJ06U8s4999yEc8aMGSPVOu2002LGR44cadu2beuwvaCgQKqn3r8q2fehind9UVRUZIcOHeqwPdn3oQqHw1Le8ePHpbw/f/fVzaRJk2zjxo2uMa/t8WzatCnhnOLiYvu3f/s3z3jCR0RZWZk1NDS0/7+hoaH9zywAAEg15ikAQDIl3FCNHTvWPv30U6utrbVIJGIrV660yZMnd8fYAABIGPMUACCZEv6Tv8zMTFu0aJHNnz/fgsGgTZkypcv+TAMAgBPFPAUASKZON1SvvfZa+78nTpxoL7/8crcMCAAABfMUACAVtG/VAQAAAAAS/5M/INnirVLjFVdXxVFX+bvhhhsSzikrK5NqffEL9268ViCKtRpVLOrS9IFAQMprbm6W8goLC6W4uqqguuqeuiJVY2OjlNenT5+Yca/xvP/++1K98vLyhHPUfQJ0FXWVwnhzjVdcXdFOXZp64MCBCecoq9eamWVlZUmPUVfPU+f7ZK+e15k8t8ckeyXaZN+OJt51l1e8ra1NqhcKhRLOibfyIZ9QAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiPypHsCpyOfzSXmBQCBmPCMjo0vrhcNhKS8ajUp5yRYKhZJab+vWrQnnBINBqVa8Y8Urnp6eLtWLRCJSXklJiZSnHpvxXnOveGZmplRPHaffr516s7Ozpbx+/fpJ8ccff1yqN2vWrIRz4h3Tyebz+TzPrV7bHceRaynUeicLdb+o5zn1GIx3nvOKV1RUSPWGDRsm5SnnY/Uc7nW9Eu8xJ8sxrV4HdWbOUOcVN2lp2ucm6uugXivEO1684sXFxVK9wYMHJ5xTVFQUM84nVAAAAAAgoqECAAAAABENFQAAAACIaKgAAAAAQERDBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACJ/qgdwKnIcR8oLhUInFD9VTZ48WYr//ve/747heFq1alXCOcePH5dqtbS0xIw3Nze7bs/IyJDqqcd0Q0ODlDd06FAp7+DBgzHjra2trtsDgYBULysrS8rLzc2V8g4fPizl7dy50zM2ZswYz/iePXukemPHjk04p6KiwmpqaqR63cHn85nP5/OMJbK9M7UU0WhUylOp54G0tOT+7lZ9P/ft21fKGzRokBQ/99xzpXqlpaVSXlFRUcI56mvXmWPa7TFqvWTnqfz++Jfdbo9Rj2n1+UUiESlPHWe8/eIVV8fpdY0US3Z2dsw4n1ABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAEQ0VAAAAAAgoqECAAAAAJE/1QNIhgkTJkh5b731VhePJLaioiIpPnDgQKne8OHDpTy13rXXXivljRgxImZ82bJlrtuXL18u1bv44oulvObm5oRzDh48KNXKyMiIGW9ra3Pd7vdrb/lIJCLlKfvEzKy1tVXKy8nJiRnPyspy3R4Oh6V68V4HL+rrnp6eLuWNGTNGih89elSqdyrw+Xzm8/k8Y4ls70wtheM4Up4q3jjT0tx/R6s+P/V4LygokPLKy8ulvHPPPVeKn3HGGVI9NS87OzvhHK/XtLsk+5hWqfulM3luj+nOem7UawW1Xry51CuemZkp1fO6FoglXi0+oQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABD5Uz2ARKSnp0t5S5culfKOHj0q5RUXF0t5hYWFMePvvvuu6/ZIJCLVU/fnkSNHpLy2tjYpr7GxUYpPmTJFqldQUCDlNTc3J5zTr18/qdbevXtjxr32SWtrq1QvIyNDyispKZHyQqGQlBeP1zGflqb9bkl5zU+E+jo0NTV5xvLy8jzj1157rVRvxYoVUl5Pkp6e7nm8eG33+XzdOaQO1OO2u8bpdXyqc02fPn2kvMGDB0t5EydOlPLKy8uleLw8L9nZ2VJeVlZWwjnqa9eZY1M9ft04jtNlP6sz1OuZzlyvuT1Gvc5TJbueSn0dlGuhYDAYM84nVAAAAAAgoqECAAAAABENFQAAAACIaKgAAAAAQNSphqqpqckqKyutrq7OzMzuvfdemzZtml111VV21VVX2bp167p1kAAAxMI8BQBIlbir/G3ZssXuu+8+q6mpad+2detWe/rpp+UVvAAA6CrMUwCAVIr7CdVzzz1nCxcubJ+UWlpabO/evbZgwQKrqqqypUuXWjQa7faBAgDghnkKAJBKPqeTi/dfdtll9t///d/mOI4tWrTIFi5caPn5+XbrrbdaZWWlXX/99d09VgAAPDFPAQBSIeEb+5aXl9vjjz/e/v85c+bYSy+9lPBENWTIEKutre2w3XEcz5sOqjeYe/PNN6W8nnRj34qKCtf9ZXbq39g3JyfHM3bWWWfZRx995BpTbw6r3tg3HA4nnKPeoDHWjX0vvPBC+8Mf/uAaCwQCUj31hrLqjYu748a+sd5D6k0h1WNazVOPl1ivQ6wb+77wwgtSvW984xsJ51RUVHzpT/ZORFfMUyNGjHA9XoLBoGVmZrrmJPvGvqruGGdLS4vn8Xmq39i3oqLCM3bHHXfYo48+6ho799xzpXr9+/eX8mLNpV78/oQvE80s/mteWlpq9fX10s924/WejEd9fuo5PN51QnFxsTU0NHTYrs7B6nsv3k1svbS0tEh5Bw8e9IyNHTvWtmzZ4hp76623pHpe10ix9OvXz37yk594xhNe5W/79u326quvtv/fcRz5gAQAoKsxTwEAkinhhspxHHvooYfs6NGjFg6H7dlnn7WpU6d2x9gAAEgY8xQAIJkS/pXdyJEj7ZZbbrGZM2daW1ubTZs2zSorK7tjbAAAJIx5CgCQTJ1uqF577bX2f8+ePdtmz57dLQMCAEDBPAUASIWT6o/K586dK+XF+sJoLJ988omUF2txiVjiLS7hFW9ubpbqqdQvR6qLZ8T7kqP6JUgvjY2NUp4yjurqaqlWvC/Xjx492nX7pk2bpHrql7wHDBgg5an+93//1zNWUVFhmzdvdo3l5eVJ9SZPnizlqUt4qwvlxPsitFd8woQJUr1TQZ8+fayoqMg15rVd/QJ4bm6ulJeWlvBf7ZuZWVZWlpQXz8iRI123q4tLqOeP888/X8pTrxXiLUQ1fvx41+3q4hLq8aIuSpRM6nWCukiEuiCROs7OLPagLgjhRj0nqc9PndviLSriFVfPEeXl5QnnxLu2187GAAAAAAAaKgAAAABQ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAEQ0VAAAAAAg8qd6AInYv3+/lLd7924pLz8/X8o7duyYlJeXlxcz7vP5XLcXFRVJ9VSBQEDK69Onj5QX7/Xz+rl79+6V6g0cOFDKq6ioSDjn3XfflWrl5ORI8TPOOEOq19raKuWtX79eytu1a5eUN3z48Jhxr2PlnHPOkeqp55bm5mYpLzs7W8qLd24JhUKu25Vj+lSRn59vBQUFrjGv7cXFxVKtZOepx5/X8/6ziRMnSnleBg0aJOWVlZVJeaeddpqUF+/95TVHFxYWSvUyMjKSlheJRKRaXtcrX+R2PlPrpaenS3mO40h50WhUyuvMON0ek5amff7RmdfBjbpf1HptbW1SXL3ePnLkSMI58V4DPqECAAAAABENFQAAAACIaKgAAAAAQERDBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ+VM9gETs2bNHynMcR8qrq6uT8nJzc6W8w4cPe8ZOP/102717t2ssLy9PqpeRkSHlFRQUSHmHDh2S8mpraz1jw4YNs08++cQ1VlxcLNXbv3+/lKfUe/7556VaK1as8IytWrXKrrjiCtfYkCFDpHrqMXbuuedKeZdeeqmUl5YW+3dEgwcPdt3++eefS/VCoZCUFwgEpLx4z89LOByW4g0NDVK9U0G/fv0890tJSYnr9rFjx0q1LrjgAilPndvS09OlvOzs7Jjxyy+/3HX7gAEDpHrqHJWTkyPlqXN3vHqlpaWu2+PtTy+ZmZlSns/nSzgnEol0W62srKwO29ra2qR6qnjnRi/qubgzeerPdhONRqU85Vg5EfGes1fc79faGGUOjleLT6gAAAAAQERDBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABE/lQPIBGbN2+W8pYvXy7lzZs3T8rbu3evlNe/f38pfuTIEanegQMHpLyGhgYpLy1N69/79u0rxbOysqR6+fn5Ul5dXV3COS+++KJU64MPPogZX7Bggev2IUOGSPWKioqkvFAoJOWpx3Q4HI4Zj0Qirtt9Pp9UTz1WMjIypDyV1/OOFw8EAlK98vLyhHMGDhwo1eou/fv3N8dxXGOlpaWu2wcNGiTVUs9VZWVlUp76usY73s8880zX7dnZ2VI9NU+da9TXId77OTc313W7369dgqnnK2W/qPuyM9Tn7yYajUp56enpUl68ucZLvHOx12O8zkXdRX1+at6xY8ekuHo9evDgQSkvFj6hAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAEQ0VAAAAAAgoqECAAAAABENFQAAAACIaKgAAAAAQERDBQAAAAAiGioAAAAAEPlTPYBkePjhh6W8zZs3S3nf+973pLxdu3Z5xs4++2zPuN+vvYx5eXlSXnZ2tpSXkZEh5aWnp8eM5+bmum4PBoNSvUgkIuU5jpNwzoEDB6RaZ511lhQ/fvy4VG/37t1S3uHDh6W8zMxMKS/W/hw+fLjV1NS4xsLhsFRv0KBBUl4gEJDyotGolOfz+WLGvY559T07YsSIhHNKS0ulWt1lwIABnseh1+uuPoeysrKk5qnvr6ysrJjx4uJiKc9LvHO/l7Q07XfF6lwa7/2lvt+9qHOUsj/jPbcTob5ObtRzYygUkvLa2tqkvJaWFukx6ntB3S/d+fzcxLs28Yqrc1Tfvn0TzunTp0/MOJ9QAQAAAICIhgoAAAAARDRUAAAAACDqVEP12GOP2YwZM2zGjBm2ePFiMzOrrq62qqoqmzZtmj3yyCPdOkgAALwwRwEAUiluQ1VdXW0bN260F1980V566SX705/+ZCtXrrQFCxbYE088YatWrbKtW7fahg0bkjFeAADaMUcBAFItbkNVXFxs99xzj2VkZFggELBhw4ZZTU2NVVRUWHl5ufn9fquqqrI1a9YkY7wAALRjjgIApFrchmr48OE2btw4MzOrqamx1atXm8/n+9LyqCUlJVZfX99tgwQAwA1zFAAg1XxOJ2+es2PHDrv11ltt/vz5lp6ebq+//rr95Cc/MTOzN954w/7rv/7LnnzyyW4dLAAAbpijAACp0qm72L3zzjv2ne98xxYsWGAzZsywt99+2xoaGtrjDQ0NVlJSklDhIUOGWG1tbYftjuN0643kEvH1r39dylNv7Nvc3OwZq6ystJUrV7rGevONfc8++2z78MMPXWPqDfvU/anc2DcnJ0eqVVhY6Bnr16+fHTx40DWm3thXvVlfT7qx79SpU23dunWusWTf2Dc/P1/K644b+w4dOtTzpuHqe33OnDkJ55SWltqyZcuket0xR91///126NChDtt//vOf25133umaM3bs2IRq/NmwYcOkvJ50Y9/S0lLPTwF78419A4GAfH7xop4HlDk42ddjyb7Rbmtrq5SnjjPeHDxs2DD75JNPOmw/1W/sG2vuvvzyy239+vWusZ07d0r1tm/fnnBO37597Yc//KFnPO5ZZ9++fXbbbbfZkiVLbMaMGWb2f5PGp59+arW1tRaJRGzlypU2efLkhAcHAMCJYI4CAKRa3F/HPPnkkxYMBm3RokXt22688UZbtGiRzZ8/34LBoE2ZMsWmT5/erQMFAOCrmKMAAKkWt6G677777L777nONvfzyy10+IAAAOos5CgCQatofGgMAAAAAOrcoRU+hftFU/VLe6tWrpTz1S44PP/xwzPgXlwH+on379kn11AUK1C9HqotSxPqi8Nlnn211dXWuMXWc6pdwlUUp1IVBYr3mkyZNso8++sg1pozRzCwSiUh56pe81fdQvP3pFVe/YFxTUyPlqV9SVxeJiLd4wZEjR1y3P/HEE1I9ry8Qx1JRUSHV6i7Hjx+3pqYm15jXdnURFq9FZOJRFzdRxTuHe8216nlH1R2Lt8QS6/wYCATk86eXYDAo5Snj6I59YvZ/52K391GsxbliUc/hap46zs5wW9ghmYtlmelzlHpsxlqUIlb8888/l+opc3e8a2Y+oQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABD5Uz2ARESj0VQPoVN++9vfSnkTJkzwjDmO4xnftWuXVK9Pnz5SXlqa1oenp6dLeX5/7MP0kksucd0eiUSkeqr9+/cnnFNXVyfVCgQCMeNe+zojI6Nb6nnx+XxSnipevfz8fNft9fX1Ur3c3Fwpr6GhQcpTX4d4eV5xdb+cCg4cOOD5nvbaLwUFBVKt7OxsKU89/lpbW6W8eOf+48ePu25va2uT6uXl5Ul56hwVDAalvJaWFs9YVlaWNTY2usbUcR4+fFjKU84f6msXL+/MM8+0PXv2dNiunhvVfen12sSjXs/k5OTEfcyxY8c6bDtZrn+PHDki5R09elSKux1DnaFcH8Z7DfiECgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAEQ0VAAAAAAgoqECAAAAABENFQAAAACIaKgAAAAAQORP9QBw4oYOHSrljRw5Usrr37+/lHfkyBEpb9CgQZ6x1atX27XXXusaq6mpkeqFw2Ep75NPPpHyFBs2bIgZ93oOwWBQqtfU1CTlpaenS3mqUCjkGRs1apTna9Tc3CzVS0vTfic1YMAAKa+srEzKe//99z1jo0ePtm3btrnGnn76aaneqWDfvn22b98+11hdXZ3r9mPHjkm1Dhw4IOXt2bNHyisqKpLyMjIyPGO33367rVmzxjVWXl4u1VPPH8XFxVKeeu6PNc7i4mLbuXOna0w9XlpbW6W8lpaWhHOi0ahUq6GhIWb8zDPPdD1e1HNxrHN/LHl5eVKe369dPg8ePDjuY+rr66Wf7SYQCEh56rWCek7yOqf+mdd76NChQ1K9zz77LOGceOcHPqECAAAAABENFQAAAACIaKgAAAAAQERDBQAAAAAiGioAAAAAENFQAQAAAICIhgoAAAAARDRUAAAAACCioQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ+VM9AKTOtm3bUj2ETtm6dWvM+Jo1a5I0kp5jypQpnjHHcWLGeyvHcezqq69O9TDMzOyb3/ymlPfRRx9JedXV1Z4xx3Hs+uuvl37uqWzPnj22e/du11htba3r9oyMDKnWoUOHpDyvccQTjUalvPz8fM/Y7bffbqtWrXKNFRYWSvX69esn5cUaZyzZ2dlSXnNzs2ds4sSJ9vLLL7vGHMeR6rW1tUl59fX1CeccO3ZMqtXS0hIzfscdd9grr7zSYfvhw4elerm5uVJeQUGBlNe3b18pLz09Pe5j9uzZ02FbKBSS6mVlZUl5TU1NUt6OHTukvIMHD8aM79y503X7xx9/LNU7fvx4wjnxjjE+oQIAAAAAEQ0VAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABD5Uz0AAOhtnnzyyVQPAXFEIhFra2tzjXltD4fDUq309HQp7/Dhw1JeVlaWlFdbWxszvnXrVtftubm5Uj11v5SVlUl5R48elfIyMzNjxl9//XXX7cePH5fq9evXT8qrr69POCcUCkm1Ghsb4z7G7XhxHEeqF41GpbyCggIpTx1ncXFx3MfU1NR02KaeWwKBgJSnvhf27Nkj5e3evTtmfMeOHa7b1XG2trYmnNPc3BwzzidUAAAAACCioQIAAAAAEQ0VAAAAAIg69R2qxx57zFavXm1mZlOmTLEf/OAHdu+999o777xj2dnZZmZ2++2329SpU7tvpAAAuGCOAgCkUtyGqrq62jZu3Ggvvvii+Xw+u/nmm23dunW2detWe/rpp62kpCQZ4wQAoAPmKABAqsX9k7/i4mK75557LCMjwwKBgA0bNsz27t1re/futQULFlhVVZUtXbpUXmEFAAAVcxQAINV8TgJrP9bU1NjMmTNt2bJl9tOf/tQWLlxo+fn5duutt1plZaVdf/313TlWAAA8MUcBAFKh0w3Vjh077NZbb7X58+fbNddc86XYunXr7KWXXrLHH3+804WHDBniek8Lx3HM5/N1+uf0FuwXd+yXjtgn7tgv7nrKfqmoqHC9/0pndfUcNWrUKPvss886bD927JjnvWvUe9Oo98JRXzf1PlSx7kn02Wef2eDBg11jvfk+VBs3brRJkya5xnrzfaj27t1rAwcO7LD9VL8P1dVXXx0zvmTJEvve977XYXtvvg/VBx98YKNHj3aNHTlyRKqn3IeqvLzc3n33Xc94p1b5e+edd+ymm26yu+++26655hrbvn27vfrqq+1xx3HM7+cewQCA5GOOAgCkUtyGat++fXbbbbfZkiVLbMaMGWb2f5PTQw89ZEePHrVwOGzPPvssqycBAJKOOQoAkGpxf2X35JNPWjAYtEWLFrVvu/HGG+2WW26xmTNnWltbm02bNs0qKyu7daAAAHwVcxQAINXiNlT33Xef3Xfffa6x2bNnd/mAAADoLOYoAECqdeo7VAAAAACAjviWLgAAXxEOhz1X1vLarq40dvjwYSlPXWks1qp0sbS1tcWMHzp0KKHt8aSlab/zVfdnS0uLlBfPhx9+6Lpdff3Ulc2UldvUVf7iHStm7seFurKj+t5TV1pU7dq1S3pMU1OTVE9dHVCtpx4vBw8elOLqe0jJi5fDJ1QAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABDRUAEAAACAiIYKAAAAAEQ0VAAAAAAgoqECAAAAABENFQAAAACIaKgAAAAAQERDBQAAAAAif6oHAABAT+P3+83vd58ivbY7jiPVikajUl56erqUp/L5fCcUT5S6P5ubm6W8SCQi5cXT1tbWpT+vsbGxR4wjlrS0+L+vd3uM+l5Qjz31NVffe7t375YeEwqFpHrq8wsGg1JeOByW8uKN0yuuvu75+fkJ5+Tm5saM8wkVAAAAAIhoqAAAAABAREMFAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABD5U1V40KBBnrGKiookjuTkwX5xx37piH3ijv3irifsl1hzQioMHDjQMzZ48GDX7Y7jSLWi0aiUl5am/U7U79em/kgkEjPutV+SLT09XcpTX4d4unq/BAIBKS8cDieco+6TzuS57Rf1PeTz+aQ8tZ56jA0YMEB6jPLamcV/z3oJhUJSXltbm5TX2toaM15eXu66XX3dMzMzE84pKyuLGfc56tEEAAAAAL0cf/IHAAAAACIaKgAAAAAQ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAEBEQwUAAAAAIhoqAAAAABD1mIZqxYoVdsUVV9i0adNs2bJlqR5OjzFnzhybMWOGXXXVVXbVVVfZli1bUj2klGpqarLKykqrq6szM7Pq6mqrqqqyadOm2SOPPJLi0aXGV/fJvffea9OmTWs/ZtatW5fiESbfY489ZjNmzLAZM2bY4sWLzYxjxcx9v3C8dB7zlDvmqb9gjnLHPNUR85S7k3aecnqAzz//3Ln00kudw4cPO8ePH3eqqqqcHTt2pHpYKReNRp1JkyY54XA41UPpETZv3uxUVlY655xzjrN7926npaXFmTJlivPZZ5854XDYmTdvnvO73/0u1cNMqq/uE8dxnMrKSqe+vj7FI0udN954w7nhhhucYDDohEIhZ+7cuc6KFSt6/bHitl/Wrl3b64+XzmKecsc89RfMUe6YpzpinnJ3Ms9TPeITqurqapswYYIVFhZaTk6Ofe1rX7M1a9akelgpt2vXLjMzmzdvnl155ZX29NNPp3hEqfXcc8/ZwoULraSkxMzM3n//fauoqLDy8nLz+/1WVVXV646br+6TlpYW27t3ry1YsMCqqqps6dKlFo1GUzzK5CouLrZ77rnHMjIyLBAI2LBhw6ympqbXHytu+2Xv3r29/njpLOYpd8xTf8Ec5Y55qiPmKXcn8zzVIxqq/fv3W3Fxcfv/S0pKrL6+PoUj6hmOHTtmEydOtMcff9yeeuope+aZZ+yNN95I9bBS5sEHH7Tzzjuv/f8cNx33yYEDB2zChAn20EMP2XPPPWebNm2yF154IYUjTL7hw4fbuHHjzMyspqbGVq9ebT6fr9cfK2775ZJLLun1x0tncb5xxzz1F8xR7pinOmKecncyz1M9oqGKRqPm8/na/+84zpf+31uNHz/eFi9ebPn5+VZUVGTXXXedbdiwIdXD6jE4bjoqLy+3xx9/3EpKSiw7O9vmzJnTa4+ZHTt22Lx58+wHP/iBlZeXc6z8f1/cL0OHDuV46STON+6Yp7xxzLhjnvoL5il3J+M81SMaqrKyMmtoaGj/f0NDQ/tHw73Zpk2b7M0332z/v+M45vf7UziinoXjpqPt27fbq6++2v7/3nrMvPPOO3bTTTfZ3Xffbddccw3Hyv/31f3C8dJ5HEPumKe8ccy447zzf5in3J2s81SPaKguuugie/PNN+3QoUPW0tJia9eutcmTJ6d6WCnX2NhoixcvtmAwaE1NTfbiiy/a1KlTUz2sHmPs2LH26aefWm1trUUiEVu5cmWvP24cx7GHHnrIjh49auFw2J599tled8zs27fPbrvtNluyZInNmDHDzDhWzNz3C8dL5zFPuWOe8sZ5xx3nHeYpLyfzPNUjWrzS0lK76667bO7cuRYOh+26666zMWPGpHpYKXfppZfali1b7Oqrr7ZoNGqzZs2y8ePHp3pYPUZmZqYtWrTI5s+fb8Fg0KZMmWLTp09P9bBSauTIkXbLLbfYzJkzra2tzaZNm2aVlZWpHlZSPfnkkxYMBm3RokXt22688cZef6x47Zfefrx0FvOUO+Ypb8xR7pinmKe8nMzzlM9xHCfVgwAAAACAk1GP+JM/AAAAADgZ0VABAAAAgIiGCgAAAABENFQAAAAAIKKhAgAAAAARDRUAAAAAiGioAAAAAED0/wB9ihWkmW+uKwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im=train_dataset[0][0][0]\n",
    "rec_im=decoder(encoder(train_dataset[0][0].reshape([1,1,28,28])))[0][0].detach()\n",
    "plot_result(im,rec_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b3bc3",
   "metadata": {},
   "source": [
    "# Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete','duration','number'], axis=1)\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf85e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61382628",
   "metadata": {},
   "source": [
    "# Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2182913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "device = \"cpu\"\n",
    "### Initialize the two networks\n",
    "encoded_space_dim = 2\n",
    "encoder = Encoder(encoded_space_dim=encoded_space_dim,device=device)\n",
    "decoder = Decoder(encoded_space_dim=encoded_space_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d35aba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 1/10 - loss: 0.180603\n",
      "\n",
      "\n",
      "EPOCH 2/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 2/10 - loss: 0.180447\n",
      "\n",
      "\n",
      "EPOCH 3/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 3/10 - loss: 0.180456\n",
      "\n",
      "\n",
      "EPOCH 4/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 4/10 - loss: 0.180542\n",
      "\n",
      "\n",
      "EPOCH 5/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 5/10 - loss: 0.180426\n",
      "\n",
      "\n",
      "EPOCH 6/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 6/10 - loss: 0.180521\n",
      "\n",
      "\n",
      "EPOCH 7/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 7/10 - loss: 0.180550\n",
      "\n",
      "\n",
      "EPOCH 8/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 8/10 - loss: 0.180441\n",
      "\n",
      "\n",
      "EPOCH 9/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 9/10 - loss: 0.180456\n",
      "\n",
      "\n",
      "EPOCH 10/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 10/10 - loss: 0.180493\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder,decoder,train_loss,test_loss = training_loop_den(10,device,encoder,decoder,train_dataloader,test_dataloader,loss_fn,optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b2086",
   "metadata": {},
   "source": [
    "# Supervised learning application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e579e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv_model import ConvolutionalNet\n",
    "classifierNet = ConvolutionalNet(act_func=nn.ReLU(), linear_size=120, conv0_size = 10, conv1_size = 20, dropout=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffb3d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_skorch = NeuralNetClassifier(\n",
    "            module=classifierNet,\n",
    "            module__act_func=nn.ReLU(),\n",
    "            criterion= nn.CrossEntropyLoss,\n",
    "            max_epochs=20,\n",
    "            optimizer__lr = 1e-3,\n",
    "            optimizer = torch.optim.Adam,\n",
    "            batch_size = 1000,\n",
    "            # optimizer__weight_decay=0.01, # L2 Regularization          \n",
    "            # train_split\n",
    "            # device='cuda',  # uncomment this to train with CUDA\n",
    "            callbacks = [callbacks.EarlyStopping(monitor='valid_loss', patience=100, threshold=0.005)],     \n",
    "            verbose = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7838db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ConvolutionalNet(\n",
       "    (act_func): ReLU()\n",
       "    (final_func): Softmax(dim=1)\n",
       "    (dropout_func): Dropout(p=0, inplace=False)\n",
       "    (conv0): Conv2d(1, 10, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
       "    (conv1): Conv2d(10, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (lin_layer): Linear(in_features=1280, out_features=100, bias=True)\n",
       "    (out_layer): Linear(in_features=100, out_features=10, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = train_dataset.data.type(torch.float32).reshape(60000,1,28,28)\n",
    "label_data = train_dataset.targets.type(torch.int64).reshape(60000)\n",
    "net_skorch.fit(input_data,label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79ea7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01aee599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Filippo\\Documents\\POD\\Projects\\NNDL\\HW2\\training_func.py:177: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(classifierNet.forward(decoded_data), torch.nn.functional.one_hot(torch.tensor(image_label),num_classes=10).type(torch.float32))\n",
      "c:\\Users\\Filippo\\Documents\\POD\\Projects\\NNDL\\HW2\\training_func.py:209: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_loss = loss_fn(classifierNet.forward(conc_out), torch.nn.functional.one_hot(torch.tensor(conc_label),num_classes=10).type(torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 1/10 - loss: 2.300492\n",
      "\n",
      "\n",
      "EPOCH 2/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 2/10 - loss: 2.298317\n",
      "\n",
      "\n",
      "EPOCH 3/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 3/10 - loss: 2.296957\n",
      "\n",
      "\n",
      "EPOCH 4/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 4/10 - loss: 2.296364\n",
      "\n",
      "\n",
      "EPOCH 5/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 5/10 - loss: 2.295939\n",
      "\n",
      "\n",
      "EPOCH 6/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 6/10 - loss: 2.295702\n",
      "\n",
      "\n",
      "EPOCH 7/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 7/10 - loss: 2.295569\n",
      "\n",
      "\n",
      "EPOCH 8/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 8/10 - loss: 2.295240\n",
      "\n",
      "\n",
      "EPOCH 9/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-debacc16e0dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_loop_sup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifierNet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Filippo\\Documents\\POD\\Projects\\NNDL\\HW2\\training_func.py\u001b[0m in \u001b[0;36mtraining_loop_sup\u001b[1;34m(num_epochs, device, encoder, decoder, train_dataloader, test_dataloader, loss_fn, optim, classifierNet)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'EPOCH %d/%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m### Training (use the training function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         train_epoch_sup(\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mdecoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Filippo\\Documents\\POD\\Projects\\NNDL\\HW2\\training_func.py\u001b[0m in \u001b[0;36mtrain_epoch_sup\u001b[1;34m(encoder, decoder, device, dataloader, loss_fn, optimizer, classifierNet, train_loss_hist)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mencoded_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# Decode data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mdecoded_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;31m# Evaluate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifierNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Filippo\\Documents\\POD\\Projects\\NNDL\\HW2\\decoders.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# Apply transposed convolutions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    921\u001b[0m             input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         return F.conv_transpose2d(\n\u001b[0m\u001b[0;32m    924\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             output_padding, self.groups, self.dilation)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder,decoder,train_loss,test_loss = training_loop_sup(10,device,encoder,decoder,train_dataloader,test_dataloader,loss_fn,optim, classifierNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df806f80",
   "metadata": {},
   "source": [
    "# Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses plot\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "sns.lineplot(x=np.arange(len(train_loss)),y=train_loss, label='Train loss', markers=True,  ax=ax)\n",
    "sns.lineplot(x=np.arange(len(val_loss)),y=val_loss, label='Validation loss',markers=True, ax=ax)\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the encoded representation of the test samples\n",
    "encoded_samples = []\n",
    "for sample in tqdm(test_dataset):\n",
    "    img = sample[0].unsqueeze(0).to(device)\n",
    "    label = sample[1]\n",
    "    # Encode image\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img  = encoder(img)\n",
    "    # Append to list\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    encoded_sample['label'] = label\n",
    "    encoded_samples.append(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "encoded_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecbe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "label_names=['t-shirt','trouser','pullover','dress','coat','sandal','shirt',\n",
    "             'sneaker','bag','boot']\n",
    "px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', color=[label_names[l] for l in encoded_samples.label.to_numpy()], opacity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed261d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoded_space_dim == 2:\n",
    "    # Generate a custom sample\n",
    "    custom_encoded_sample = [-6, -30.0]\n",
    "    encoded_value = torch.tensor(custom_encoded_sample).float().unsqueeze(0).to(device)\n",
    "\n",
    "    # Decode sample\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_img  = decoder(encoded_value)\n",
    "\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(generated_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6014fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "encoded_samples_reduced_TSNE = tsne.fit_transform(encoded_samples)\n",
    "plt.scatter(encoded_samples_reduced_TSNE[:,0], encoded_samples_reduced_TSNE[:,1], c=encoded_samples.label.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0679791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "encoded_samples_reduced_PCA = pca.fit_transform(encoded_samples)\n",
    "encoded_samples_reduced_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a52bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(encoded_samples_reduced_PCA[:,0], encoded_samples_reduced_PCA[:,1], c=encoded_samples.label.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be89501",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41fb9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(encoded_space_dim=4)\n",
    "decoder = Decoder(encoded_space_dim=2)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01024c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 1/10 - loss: 0.155250\n",
      "\n",
      "\n",
      "EPOCH 2/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 2/10 - loss: 0.155236\n",
      "\n",
      "\n",
      "EPOCH 3/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 3/10 - loss: 0.155263\n",
      "\n",
      "\n",
      "EPOCH 4/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 4/10 - loss: 0.155225\n",
      "\n",
      "\n",
      "EPOCH 5/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 5/10 - loss: 0.155191\n",
      "\n",
      "\n",
      "EPOCH 6/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 6/10 - loss: 0.155208\n",
      "\n",
      "\n",
      "EPOCH 7/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 7/10 - loss: 0.155244\n",
      "\n",
      "\n",
      "EPOCH 8/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 8/10 - loss: 0.155210\n",
      "\n",
      "\n",
      "EPOCH 9/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 9/10 - loss: 0.155222\n",
      "\n",
      "\n",
      "EPOCH 10/10\n",
      "\n",
      "\n",
      "\t VALIDATION - EPOCH 10/10 - loss: 0.155204\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder,decoder,train_loss,test_loss = training_loop_autoenc(10,device,encoder,decoder,train_dataloader,test_dataloader,loss_fn,optim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
